{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_lab6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JtIFxFPzrRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNH6ZFgYBflK",
        "colab_type": "code",
        "outputId": "027189f7-380d-46e0-8a8f-a12e63aba077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9W41eA4BZ7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['He is a good person',\n",
        "          'He is bad student',\n",
        "          'He is hardworking']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwp5atSp730s",
        "colab_type": "code",
        "outputId": "89d5beba-2bfc-4abb-a056-5dc2c5330e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "word_frequency = {}\n",
        "for sentence in corpus:\n",
        "  sentence = sentence.split(' ')\n",
        "  for word in sentence:\n",
        "    if word in word_frequency:\n",
        "      word_frequency[word]+=1\n",
        "    else:\n",
        "      word_frequency[word] =1 \n",
        "word_frequency\n",
        "pd.DataFrame(data = word_frequency,index = ['Frequency'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>He</th>\n",
              "      <th>is</th>\n",
              "      <th>a</th>\n",
              "      <th>good</th>\n",
              "      <th>person</th>\n",
              "      <th>bad</th>\n",
              "      <th>student</th>\n",
              "      <th>hardworking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Frequency</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           He  is  a  good  person  bad  student  hardworking\n",
              "Frequency   3   3  1     1       1    1        1            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0J1lrR0hyy",
        "colab_type": "code",
        "outputId": "a8ae5de2-d766-49da-832d-14b4f3e16b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "df = pd.DataFrame(data=corpus, columns=['sentences'])\n",
        "\n",
        "vectorizer = CountVectorizer(vocabulary=['he', 'is', 'a', 'good', 'person', 'bad', 'student', 'hardworking'], min_df=0,\n",
        "                             stop_words=frozenset(), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "X = vectorizer.fit_transform(df['sentences'].values)\n",
        "result = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names())\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   he  is  a  good  person  bad  student  hardworking\n",
            "0   1   1  1     1       1    0        0            0\n",
            "1   1   1  0     0       0    1        1            0\n",
            "2   1   1  0     0       0    0        0            1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29CyLmEy_NNc",
        "colab_type": "code",
        "outputId": "38990102-97f3-4566-c47c-507f873c19f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITTDs6jG0rOL",
        "colab_type": "code",
        "outputId": "f7cb7865-ac3c-4097-d5e3-383414c1fb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import warnings \n",
        "  \n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "  \n",
        "import gensim \n",
        "from gensim.models import Word2Vec \n",
        "  \n",
        "#  Reads ‘alice.txt’ file \n",
        "sample = open(\"/content/drive/My Drive/Datasets/token.txt\", \"r\") \n",
        "s = sample.read() \n",
        "  \n",
        "# Replaces escape character with space \n",
        "f = s.replace(\"\\n\", \" \") \n",
        "  \n",
        "data = [] \n",
        "  \n",
        "# iterate through each sentence in the file \n",
        "for i in sent_tokenize(f): \n",
        "    temp = [] \n",
        "      \n",
        "    # tokenize the sentence into words \n",
        "    for j in word_tokenize(i): \n",
        "        temp.append(j.lower()) \n",
        "  \n",
        "    data.append(temp) \n",
        "  \n",
        "# Create CBOW model \n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
        "                              size = 100, window = 5) \n",
        "  \n",
        "# Print results \n",
        "print(\"Cosine similarity between 'attracted' \" + \n",
        "               \"and 'communicates' - CBOW : \", \n",
        "    model1.similarity('attracted', 'communicates')) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'attracted' and 'communicates' - CBOW :  -0.06914011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHeDT8lb-_Yb",
        "colab_type": "code",
        "outputId": "875e4fca-d512-4ad5-aff9-d5be52f4294d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# define example\n",
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
        "values = array(data)\n",
        "print(values)\n",
        "# integer encode\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "# binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7muEWPwCLad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNccOtpU_sYN",
        "colab_type": "code",
        "outputId": "532a2984-48f4-4524-c27e-d6c5c296f0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "len(documents)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9G6lO_HCQjv",
        "colab_type": "code",
        "outputId": "80d752e5-5463-4b87-ca6a-51661ed4ae40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "dataset.target_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTcR1dGgCUsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df = pd.DataFrame({'document':documents})\n",
        "\n",
        "# removing everything except alphabets`\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# removing short words\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "# make all text lowercase\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoFEhxQSCWzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# tokenization\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "\n",
        "# remove stop-words\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "# de-tokenization\n",
        "detokenized_doc = []\n",
        "for i in range(len(news_df)):\n",
        "    t = ' '.join(tokenized_doc[i])\n",
        "    detokenized_doc.append(t)\n",
        "\n",
        "news_df['clean_doc'] = detokenized_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSS3oVQKCZBa",
        "colab_type": "code",
        "outputId": "48e20576-a621-4efe-e009-f483687d003f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "max_features= 1000, # keep top 1000 terms \n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "\n",
        "X.shape # check shape of the document-term matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcSygWeKCjKK",
        "colab_type": "code",
        "outputId": "91f93a52-e71d-4112-a241-d6ccf8200bd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# SVD represent documents and terms in vectors \n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "\n",
        "svd_model.fit(X)\n",
        "\n",
        "len(svd_model.components_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ejv3ucC0r6",
        "colab_type": "code",
        "outputId": "5975bd29-3f06-4698-cb2c-0bb9029f0650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, comp in enumerate(svd_model.components_):\n",
        "    terms_comp = zip(terms, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"Topic \"+str(i)+\": \",end=' ')\n",
        "    for t in sorted_terms:\n",
        "        print(t[0],end=' ')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0:  like know people think good time thanks \n",
            "Topic 1:  thanks windows card drive mail file advance \n",
            "Topic 2:  game team year games season players good \n",
            "Topic 3:  drive scsi disk hard card drives problem \n",
            "Topic 4:  windows file window files program using problem \n",
            "Topic 5:  government chip mail space information encryption data \n",
            "Topic 6:  like bike know chip sounds looks look \n",
            "Topic 7:  card sale video offer monitor price jesus \n",
            "Topic 8:  know card chip video government people clipper \n",
            "Topic 9:  good know time bike jesus problem work \n",
            "Topic 10:  think chip good thanks clipper need encryption \n",
            "Topic 11:  thanks right problem good bike time window \n",
            "Topic 12:  good people windows know file sale files \n",
            "Topic 13:  space think know nasa problem year israel \n",
            "Topic 14:  space good card people time nasa thanks \n",
            "Topic 15:  people problem window time game want bike \n",
            "Topic 16:  time bike right windows file need really \n",
            "Topic 17:  time problem file think israel long mail \n",
            "Topic 18:  file need card files problem right good \n",
            "Topic 19:  problem file thanks used space chip sale \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pUtEHPFC4CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}